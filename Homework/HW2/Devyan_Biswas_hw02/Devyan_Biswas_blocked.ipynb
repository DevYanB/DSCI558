{"cells":[{"cell_type":"markdown","metadata":{"id":"rV3ck1wXJ3xL","pycharm":{"name":"#%% md\n"}},"source":["# Task 1: Using RLTK to perform Entity Resolution (ER)\n","\n","<sub>Content of this notebook was prepared by Basel Shbita, and modified by Avijit Thawani (thawani@usc.edu) as part of the class <u>DSCI 558: Building Knowledge Graphs</u> at University of Southern California (USC).</sub>"]},{"cell_type":"markdown","metadata":{"id":"vKYLgCLPJ3xN","pycharm":{"name":"#%% md\n"}},"source":["The Record Linkage ToolKit ([RLTK](https://github.com/usc-isi-i2/rltk)) is a general-purpose open-source record linkage platform that allows users to build powerful Python programs that link records referring to the same underlying entity.\n","\n","This notebook introduces some applied examples using RLTK. You can also find additional examples and use-cases in [RLTK's documentation](https://rltk.readthedocs.io/en/master/)."]},{"cell_type":"markdown","metadata":{"id":"z2Q5FfqkJ3xN","pycharm":{"name":"#%% md\n"}},"source":["## Dataset analysis & RLTK components construction"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"fTlKyPqCJ3xO","outputId":"f8d02d9d-601c-4c75-e48e-842d5dba3de7","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: rltk in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (2.0.0a20)\n","Requirement already satisfied: matplotlib>=2.0.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from rltk) (3.5.3)\n","Requirement already satisfied: scipy>=1.1.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from rltk) (1.7.3)\n","Requirement already satisfied: pandas>=1.2.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from rltk) (1.3.5)\n","Requirement already satisfied: Cython>=0.28.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from rltk) (0.29.32)\n","Requirement already satisfied: distributed>=1.23 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from rltk) (2022.2.0)\n","Requirement already satisfied: numpy>=1.17.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from rltk) (1.21.6)\n","Requirement already satisfied: pyrallel.lib in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from rltk) (0.0.10)\n","Requirement already satisfied: dask>=0.19.2 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from rltk) (2022.2.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from matplotlib>=2.0.0->rltk) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from matplotlib>=2.0.0->rltk) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from matplotlib>=2.0.0->rltk) (2.8.2)\n","Requirement already satisfied: packaging>=20.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from matplotlib>=2.0.0->rltk) (21.3)\n","Requirement already satisfied: fonttools>=4.22.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from matplotlib>=2.0.0->rltk) (4.37.2)\n","Requirement already satisfied: pillow>=6.2.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from matplotlib>=2.0.0->rltk) (9.2.0)\n","Requirement already satisfied: cycler>=0.10 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from matplotlib>=2.0.0->rltk) (0.11.0)\n","Requirement already satisfied: pytz>=2017.3 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from pandas>=1.2.0->rltk) (2022.2.1)\n","Requirement already satisfied: pyyaml in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from distributed>=1.23->rltk) (6.0)\n","Requirement already satisfied: zict>=0.1.3 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from distributed>=1.23->rltk) (2.2.0)\n","Requirement already satisfied: click>=6.6 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from distributed>=1.23->rltk) (8.1.3)\n","Requirement already satisfied: tornado>=5; python_version < \"3.8\" in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from distributed>=1.23->rltk) (6.2)\n","Requirement already satisfied: msgpack>=0.6.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from distributed>=1.23->rltk) (1.0.4)\n","Requirement already satisfied: setuptools in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from distributed>=1.23->rltk) (41.2.0)\n","Requirement already satisfied: toolz>=0.8.2 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from distributed>=1.23->rltk) (0.12.0)\n","Requirement already satisfied: cloudpickle>=1.5.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from distributed>=1.23->rltk) (2.2.0)\n","Requirement already satisfied: tblib>=1.6.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from distributed>=1.23->rltk) (1.7.0)\n","Requirement already satisfied: psutil>=5.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from distributed>=1.23->rltk) (5.9.2)\n","Requirement already satisfied: jinja2 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from distributed>=1.23->rltk) (3.1.2)\n","Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from distributed>=1.23->rltk) (2.4.0)\n","Requirement already satisfied: dill>=0.3 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from pyrallel.lib->rltk) (0.3.5.1)\n","Requirement already satisfied: typing>=3.6 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from pyrallel.lib->rltk) (3.7.4.3)\n","Requirement already satisfied: multiprocess>=0.70 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from pyrallel.lib->rltk) (0.70.13)\n","Requirement already satisfied: partd>=0.3.10 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from dask>=0.19.2->rltk) (1.3.0)\n","Requirement already satisfied: fsspec>=0.6.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from dask>=0.19.2->rltk) (2022.8.2)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->rltk) (4.3.0)\n","Requirement already satisfied: six>=1.5 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->rltk) (1.16.0)\n","Requirement already satisfied: heapdict in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from zict>=0.1.3->distributed>=1.23->rltk) (1.0.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from click>=6.6->distributed>=1.23->rltk) (4.12.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from jinja2->distributed>=1.23->rltk) (2.1.1)\n","Requirement already satisfied: locket in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from partd>=0.3.10->dask>=0.19.2->rltk) (1.0.0)\n","Requirement already satisfied: zipp>=0.5 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click>=6.6->distributed>=1.23->rltk) (3.8.1)\n","\u001b[33mWARNING: You are using pip version 19.2.3, however version 22.2.2 is available.\n","You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install rltk"]},{"cell_type":"markdown","metadata":{"id":"K8fyiwzJJ3xP","pycharm":{"name":"#%% md\n"}},"source":["### Task 1-1. Construct RLTK Datasets\n","\n","First, you need define how a single entry would like for each type of record (for each dataset)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"We7cQGTRJ3xP","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["import rltk\n","import csv\n","\n","# You can use this tokenizer in case you need to manipulate some data\n","tokenizer = rltk.tokenizer.crf_tokenizer.crf_tokenizer.CrfTokenizer()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"T3Q_rPqmJ3xP","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["'''\n","Feel free to add more columns here for use in record linkage.\n","COLS ADDED:\n","- three authors (one in each column, can use the count maybe to block)\n","- ISBN (fun fact, there are meanings to each group of 4 numbers in the ISBN13, use some grouping of that to block)\n","- publisher (we can try string matching)\n","'''\n","\n","class GoodRecord(rltk.Record):\n","    def __init__(self, raw_object):\n","        super().__init__(raw_object)\n","        self.name = ''\n","\n","    @rltk.cached_property\n","    def id(self):\n","        return self.raw_object['ID']\n","\n","    @rltk.cached_property\n","    def ISBN13(self):\n","        # Defined by student\n","        return self.raw_object['ISBN13']\n","    \n","    @rltk.cached_property\n","    def first_author(self):\n","        # Defined by student\n","        if self.raw_object['FirstAuthor'] == '':\n","            return \" \"\n","        else:\n","            return self.raw_object['FirstAuthor']\n","\n","    @rltk.cached_property\n","    def second_author(self):\n","        # Defined by student\n","        if self.raw_object['SecondAuthor'] == '':\n","            return \" \"\n","        else:\n","            return self.raw_object['SecondAuthor']\n","\n","    @rltk.cached_property\n","    def third_author(self):\n","        # Defined by student\n","        if self.raw_object['ThirdAuthor'] == '':\n","            return \" \"\n","        else:\n","            return self.raw_object['ThirdAuthor']\n","    \n","    @rltk.cached_property\n","    def publisher(self):\n","        # Defined by student\n","        return self.raw_object['Publisher']\n","\n","    @rltk.cached_property\n","    def name_string(self):\n","        return self.raw_object['Title']\n","\n","    @rltk.cached_property\n","    def name_tokens(self):\n","        return set(tokenizer.tokenize(self.name_string))\n","    \n","    @rltk.cached_property\n","    def num_authors(self):\n","        # Defined by student\n","        firstboi = self.raw_object['FirstAuthor']\n","        secondboi = self.raw_object['SecondAuthor']\n","        thirdboi = self.raw_object['ThirdAuthor']\n","        count = 0\n","        if firstboi != ' ':\n","            count+=1\n","        if secondboi != ' ':\n","            count+=1\n","        if thirdboi != ' ':\n","            count+=1\n","        return str(count)\n","\n","\n","class NobleRecord(rltk.Record):\n","    def __init__(self, raw_object):\n","        super().__init__(raw_object)\n","        self.name = ''\n","\n","    @rltk.cached_property\n","    def id(self):\n","        return self.raw_object['ID']\n","\n","    @rltk.cached_property\n","    def ISBN13(self):\n","        # Defined by student\n","        return self.raw_object['ISBN13']\n","\n","    @rltk.cached_property\n","    def first_author(self):\n","        # Defined by student\n","        if self.raw_object['Author1'] == '':\n","            return \" \"\n","        else:\n","            return self.raw_object['Author1']\n","\n","    @rltk.cached_property\n","    def second_author(self):\n","        # Defined by student\n","        if self.raw_object['Author2'] == '':\n","            return \" \"\n","        else:\n","            return self.raw_object['Author2']\n","\n","    @rltk.cached_property\n","    def third_author(self):\n","        # Defined by student\n","        if self.raw_object['Author3'] == '':\n","            return \" \"\n","        else:\n","            return self.raw_object['Author3']\n","    \n","    @rltk.cached_property\n","    def publisher(self):\n","        # Defined by student\n","        return self.raw_object['Publisher']\n","    \n","    @rltk.cached_property\n","    def num_authors(self):\n","        # Defined by student\n","        firstboi = self.raw_object['Author1']\n","        secondboi = self.raw_object['Author2']\n","        thirdboi = self.raw_object['Author3']\n","        count = 0\n","        if firstboi != ' ':\n","            count+=1\n","        if secondboi != ' ':\n","            count+=1\n","        if thirdboi != ' ':\n","            count+=1\n","        return str(count)\n","\n","    @rltk.cached_property\n","    def name_string(self):\n","        return self.raw_object['Title']\n","    \n","    @rltk.cached_property\n","    def name_tokens(self):\n","        return set(tokenizer.tokenize(self.name_string))"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"cZ2VHWnwJ3xQ","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["dir_ = ''\n","good_file = dir_ + 'goodreads.csv'\n","noble_file = dir_ + 'barnes_and_nobles.csv'\n","\n","ds1 = rltk.Dataset(rltk.CSVReader(good_file),record_class=GoodRecord)\n","ds2 = rltk.Dataset(rltk.CSVReader(noble_file),record_class=NobleRecord)"]},{"cell_type":"markdown","metadata":{"id":"4XHgnuhJJ3xQ","pycharm":{"name":"#%% md\n"}},"source":["You can load your csv files into RLTK using this method:"]},{"cell_type":"markdown","metadata":{"id":"KIylouixJ3xQ","pycharm":{"name":"#%% md\n"}},"source":["And we can inspect a few entries:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5CUrpjGTJ3xQ","outputId":"498d3b0e-a742-4740-8751-3a348661e5d0","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["  id         ISBN13     first_author   second_author     third_author  \\\n","0  0  9780340728567    Alex Ferguson                                    \n","1  1  9780844627106  Boris Pasternak                                    \n","2  2  9780712679480  Betty Boothroyd                                    \n","3  3  9780725100148           Caddie                                    \n","4  4  9780340014684   Rudolf Nureyev  Richard Avedon  Alexander Bland   \n","\n","               publisher                                 name_string  \\\n","0     Hodder & Stoughton          Managing My Life: My Autobiography   \n","1  Peter Smith Publisher     I Remember: Sketch for an Autobiography   \n","2        Random House UK              Betty Boothroyd: Autobiography   \n","3              Sun Books  Caddie, A Sydney Barmaid: An Autobiography   \n","4        E P Dutton & Co     Nureyev: An Autobiography With Pictures   \n","\n","                                         name_tokens num_authors  \n","0             {Autobiography, :, My, Managing, Life}           1  \n","1   {I, Autobiography, Remember, :, an, for, Sketch}           1  \n","2               {:, Betty, Boothroyd, Autobiography}           1  \n","3  {Autobiography, Barmaid, Sydney, :, Caddie, An...           1  \n","4    {Autobiography, Pictures, :, With, Nureyev, An}           3  \n"]}],"source":["# print some entries\n","print(ds1.generate_dataframe().head(5))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  id         ISBN13           first_author second_author  third_author  \\\n","0  0  9780984504176   Laura Ingalls Wilder                               \n","1  1  9780062376336             Chris Kyle  Scott McEwen  Jim DeFelice   \n","2  2  9780345350688              Malcolm X                               \n","3  3  9781556520747          Assata Shakur                               \n","4  4  9780876120798  Paramahansa Yogananda                               \n","\n","                               publisher num_authors  \\\n","0  South Dakota State Historical Society           1   \n","1               HarperCollins Publishers           3   \n","2          Random House Publishing Group           1   \n","3     Chicago Review Press, Incorporated           1   \n","4            Self-Realization Fellowship           1   \n","\n","                                         name_string  \\\n","0          Pioneer Girl: The Annotated Autobiography   \n","1  American Sniper (Movie Tie-in Edition): The Au...   \n","2                     The Autobiography of Malcolm X   \n","3                           Assata: An Autobiography   \n","4                            Autobiography of a Yogi   \n","\n","                                         name_tokens  \n","0  {Annotated, Autobiography, Girl, :, Pioneer, The}  \n","1  {Autobiography, -, :, Movie, Military, in, S, ...  \n","2               {of, Autobiography, Malcolm, The, X}  \n","3                     {:, An, Autobiography, Assata}  \n","4                       {a, Yogi, of, Autobiography}  \n"]}],"source":["# Printing some more entries\n","print(ds2.generate_dataframe().head(5))"]},{"cell_type":"markdown","metadata":{"id":"MB0HHqDpJ3xR"},"source":["### Task 1-2. Blocking\n","\n","First, we'll load dev set to evaluate both blocking (Task 1-2) and entity linking (Task 1-3)."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"YfGah0DhJ3xR","outputId":"762cc35a-df00-46f4-fb8f-0200a600bd71"},"outputs":[{"name":"stdout","output_type":"stream","text":["Column names are: goodreads.ID, barnes_and_nobles.ID, label\n","Processed 297 lines.\n"]}],"source":["dev_set_file = dir_ + 'dev.csv'\n","dev = []\n","with open(dev_set_file, encoding='utf-8', errors=\"replace\") as csv_file:\n","    csv_reader = csv.reader(csv_file, delimiter=',')\n","    line_count = 0\n","    for row in csv_reader:\n","        if len(row) <= 1:\n","            continue\n","        if line_count == 0:\n","            columns = row\n","            line_count += 1\n","        else:\n","            dev.append(row)\n","    print(f'Column names are: {\", \".join(columns)}')\n","    print(f'Processed {len(dev)} lines.')\n","\n","gt = rltk.GroundTruth()\n","for row in dev:    \n","    r1 = ds1.get_record(row[0])\n","    r2  = ds2.get_record(row[1])\n","    if row[-1] == '1':\n","        gt.add_positive(r1.raw_object['ID'], r2.raw_object['ID'])\n","    else:\n","        gt.add_negative(r1.raw_object['ID'], r2.raw_object['ID'])\n","\n","trial = rltk.Trial(gt)"]},{"cell_type":"markdown","metadata":{"id":"aJ07ud86J3xR"},"source":["Then, you can build your own blocking techniques and evaluate it.\n","\n","Hint:\n","\n","- What is the total number of pairs without blocking? \n","- what is the number of paris with blocking?\n","- After blocking, how many \"correct\" (matched) pairs presented in dev set?\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# name_string first few characters blocking\n","bg1 = rltk.HashBlockGenerator()\n","block1 = bg1.generate(\n","    bg1.block(ds1, function_=lambda r: r.name_string[:3]),\n","    bg1.block(ds2, function_=lambda r: r.name_string[:3])\n",")\n","# pairs = rltk.get_record_pairs(ds1, ds2, block=block1)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# number of authors blocking\n","# bg2 = rltk.HashBlockGenerator()\n","# block2 = bg2.generate(\n","#     bg2.block(ds1, property_='num_authors', base_on=block1),\n","#     bg2.block(ds2, property_='num_authors', base_on=block1)\n","# )\n","# pairs = rltk.get_record_pairs(ds1, ds2, block=block2)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# for r1, r2 in pairs:\n","#     print(r1.id, r1.name_string, '\\t', r2.id, r2.name_string)\n","\n","# for idx, b in enumerate(block1.key_set_adapter):\n","#     print(b)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# ISBN based blocking\n","bg3 = rltk.HashBlockGenerator()\n","block3 = bg3.generate(\n","    bg3.block(ds1, function_=lambda r: r.ISBN13[:3], base_on=block1),\n","    bg3.block(ds2, function_=lambda r: r.ISBN13[:3], base_on=block1)\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Further ISBN Blocking\n","# bg4 = rltk.HashBlockGenerator()\n","# block4 = bg4.generate(\n","#     bg4.block(ds1, function_=lambda r: r.ISBN13[6:9], base_on=block3),\n","#     bg4.block(ds2, function_=lambda r: r.ISBN13[6:9], base_on=block3)\n","# )"]},{"cell_type":"markdown","metadata":{},"source":["### Metrics for 1-2\n","- First, how do we get the total number of potential pairs without blocking?\n","- Should be straightforward; if there are N entries in goodreads,\n","    and M entries in barnes and nobles, then total potential pairs\n","    should just by NxM"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total possible pairs:  14681867\n"]}],"source":["total_possible_pairs = len(ds1.generate_dataframe()) * len(ds2.generate_dataframe())\n","print(\"Total possible pairs: \", total_possible_pairs)"]},{"cell_type":"markdown","metadata":{},"source":["- Alrighty then. Next, what is the number of pairs _with_ blocking?\n","- From our previous steps we have the blocks, whose entries are\n","  grouped together. So what we can try to do is see total possible pairs using\n","  the in-built functionality and by counting pairs based on the last of our\n","  sequential blocks, ie block 3"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total blocked pairs:  411599\n"]}],"source":["pairs = rltk.get_record_pairs(ds1, ds2, block=block3)\n","total_blocked_pairs = 0\n","for r1, r2 in pairs:\n","    total_blocked_pairs = total_blocked_pairs + 1\n","    # print(r1.id, r1.name_string, '\\t', r2.id, r2.name_string)\n","\n","# print('inside blocks:')\n","# for b, d, r in block3:\n","#     print(b, d, r)\n","\n","print(\"Total blocked pairs: \", total_blocked_pairs)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["pairs = rltk.get_record_pairs(ds1, ds2, block=block3)\n","with open(dir_ + 'Devyan_Biswas_blocked.csv', mode='w') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(['goodreads.ID','barnes_and_nobles.ID'])\n","    for gr, ban in pairs:\n","        writer.writerow([gr.id, ban.id])"]},{"cell_type":"markdown","metadata":{},"source":["- So, here's our efficiency/reduction-ratio\n","- It's nothing but the total compared pairs over the total possible pairs of the whole dataset"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Reduction Ratio:  0.028034513594217957\n"]}],"source":["red_ratio = total_blocked_pairs / total_possible_pairs\n","print(\"Reduction Ratio: \", red_ratio)"]},{"cell_type":"markdown","metadata":{},"source":["Woah, nice reduction ratio! But how does our blocking hold up when comparing to pair completeness or quality?\n","<br>\n","Let's test that on the dev.csv examples that we'd established earlier!"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive True matches:  67\n","Negative True matches:  230\n"]}],"source":["true_matches = 0\n","neg_matches = 0\n","for entry in dev:\n","    if entry[2] == '1':\n","        true_matches = true_matches + 1\n","    elif entry[2] == '0':\n","        neg_matches = neg_matches + 1\n","\n","\n","print(\"Positive True matches: \", true_matches)\n","print(\"Negative True matches: \", neg_matches)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive True matches compared:  52\n","Negative True matches compared:  39\n","Total pairs compared (and found from ground truth):  91\n"]}],"source":["pairs = rltk.get_record_pairs(ds1, ds2, block=block3)\n","positive_compared_matches = 0\n","negative_compared_matches = 0\n","\n","for r1,r2 in pairs:\n","    for entry in dev:\n","        if entry[2] == '1' and r1.id == entry[0] and r2.id == entry[1]:\n","            positive_compared_matches = positive_compared_matches + 1\n","        elif entry[2] == '0' and r1.id == entry[0] and r2.id == entry[1]:\n","            negative_compared_matches = negative_compared_matches + 1\n","\n","total_compared_pairs = positive_compared_matches + negative_compared_matches # needs to be the total number in DEV\n","\n","print(\"Positive True matches compared: \", positive_compared_matches)\n","print(\"Negative True matches compared: \", negative_compared_matches)\n","print(\"Total pairs compared (and found from ground truth): \", total_compared_pairs)\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Recall:  0.7761194029850746\n"]}],"source":["# Recall/ pair completeness measure: true pairs compared over total true pairs\n","recall = positive_compared_matches / true_matches\n","print(\"Recall: \", recall)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Precision:  0.5714285714285714\n"]}],"source":["# Pair quality measure: number of true pairs compared over total number of pairs compared\n","prec = positive_compared_matches/total_compared_pairs #???\n","print(\"Precision: \", prec)"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, our recall is ~0.78, and our precision is ~0.57\n","<br>\n","Note that we're measuring these with respect to our ground truths in dev.csv"]},{"cell_type":"markdown","metadata":{"id":"OCpi3AqZJ3xR","pycharm":{"name":"#%% md\n"}},"source":["### Task 1-3. Entity Linking"]},{"cell_type":"markdown","metadata":{"id":"98xt8o9kJ3xS","pycharm":{"name":"#%% md\n"}},"source":["Here are 2 example functions for field (attribute) similarity:"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"oQ3jFoFSJ3xS","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["def basic_jaro_wrinkler(r1, r2):\n","    '''Really basic jaro wrinkler to the titles'''\n","    s1 = r1.name_string[:5]\n","    s2 = r2.name_string[:5]\n","    \n","    return rltk.jaro_winkler_similarity(s1, s2)\n","\n","def basic_jaccard(r1, r2):\n","    '''Really basic jaccard to the title tokens'''\n","    s1 = r1.name_tokens\n","    s2 = r2.name_tokens\n","    return rltk.jaccard_index_similarity(s1,s2)\n","\n","# def isbn13_sim(r1, r2):\n","#     '''\n","#     Basically, check their isbns\n","#     '''\n","#     print(r1.ISBN13, '\\t', r2.ISBN13)\n","#     if r1.ISBN13 == r2.ISBN13:\n","#         print(\"HERE\")\n","#         return 1\n","#     return 0\n","\n","\n","def author_similarity(r1,r2, print_flag=False):\n","    '''\n","    I had initially done the average of all the max matches\n","    I hadnt considered the siuation in which theres a subset of the matching strings.\n","    So could turn them into tokens then do subjset, ie jaccard?\n","    OR I could just try to do the max similarity score overall, then it's all good maybe...\n","    '''\n","\n","    r1_author_list = []\n","    r1_author_list.append(r1.first_author)\n","    r1_author_list.append(r1.second_author)\n","    r1_author_list.append(r1.third_author)\n","    \n","    r2_author_list = []\n","    r2_author_list.append(r2.first_author)\n","    r2_author_list.append(r2.second_author)\n","    r2_author_list.append(r2.third_author)\n","\n","    # max_sim_score = float('-inf')\n","    # for r1a in r1_author_list:\n","    #     for r2a in r2_author_list:\n","    #         curr_sim = rltk.jaro_winkler_similarity(r1a, r2a)\n","    #         if curr_sim > max_sim_score:\n","    #             max_sim_score = curr_sim\n","    \n","    # return max_sim_score\n","\n","    cum_sum = 0\n","    for r1a in r1_author_list:\n","        max_sum = float('-inf')\n","        for r2a in r2_author_list:\n","            if  (r1a == ' ' or r2a == ' '):\n","                continue\n","            # print(\"R1 author: \", r1a)  \n","            # print(\"R2 author: \", r2a)  \n","            # print(\"---\")\n","            curr_sum = rltk.jaro_winkler_similarity(r1a, r2a)\n","            # print(curr_sum)\n","            if curr_sum > max_sum:\n","                # print(\"HERE\")\n","                max_sum = curr_sum\n","        if(max_sum != float('-inf')):\n","            cum_sum = cum_sum + max_sum\n","        # print(\"===\")\n","    \n","    # print(\"CUM SUM: \", cum_sum)\n","    # print(r1.num_authors, r2.num_authors)\n","    # print(\"MAX AUTH: \", max(int(r1.num_authors), int(r2.num_authors)))\n","    return cum_sum/max(int(r1.num_authors), int(r2.num_authors))\n","    # return cum_sum/3\n","    \n","\n","def name_string_similarity_test(r1, r2):\n","    ''' Example dummy similiary function '''\n","    s1 = r1.name_string\n","    s2 = r2.name_string\n","    \n","    if s1 == s2:\n","        return 1\n","    \n","    return 0"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["0.6862745098039215"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["rltk.jaro_winkler_similarity(' ', ' Agatha Christie ')"]},{"cell_type":"markdown","metadata":{"id":"9peeajHNJ3xS","pycharm":{"name":"#%% md\n"}},"source":["Here's how you can combine multiple similarity functions into a single weightened scoring function:"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"fGf52bBcJ3xS","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# threshold value to determine if we are confident the record match\n","MY_TRESH = 0.83 # this number is just an example, you need to change it\n","\n","# entity linkage scoring function\n","def rule_based_method(r1, r2, print_flag=False):\n","    score_1 = basic_jaro_wrinkler(r1, r2)\n","    score_2 = basic_jaccard(r1, r2)\n","    score_3 = author_similarity(r1, r2, print_flag)\n","\n","    # My current plan: conditional weights IF the scores are low/high enough on name similarity\n","    # \n","\n","    total = (0.20 * score_1) + (0.30 * score_2) + (0.50 * score_3)\n","    # print(\"SCORES: \", score_1, '\\t', score_2, '\\t', score_3, \"\\t Total: \", total)\n","\n","    # return two values: boolean if they match or not, float to determine confidence\n","    return total > MY_TRESH, total"]},{"cell_type":"markdown","metadata":{"id":"WLbrTPamJ3xS","pycharm":{"name":"#%% md\n"}},"source":["Lets run some candidates using the ground-truth"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Xo9-fdimJ3xS","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["trial = rltk.Trial(gt)\n","candidate_pairs = rltk.get_record_pairs(ds1, ds2, ground_truth=gt, block=block3)\n","for r1, r2 in candidate_pairs:\n","    result, confidence = rule_based_method(r1, r2, True)\n","    # if r1.id == '2':\n","    #     print(r1.name_string, \"\\t\", r2.name_string)\n","    #     print(r1.first_author, \"\\t\", r1.second_author, \"\\t\", r1.third_author)\n","    #     print(r2.first_author, \"\\t\", r2.second_author, \"\\t\", r2.third_author)\n","    # print(confidence)\n","    # print(\"=======\")\n","    trial.add_result(r1, r2, result, confidence)"]},{"cell_type":"markdown","metadata":{"id":"tqhybHyKJ3xS","pycharm":{"name":"#%% md\n"}},"source":["Now lets evaluate our trial results"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"7PJlbaUcJ3xS","outputId":"bce6863d-2ee2-4b6d-90cd-90029742cd60","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial statistics based on Ground-Truth from development set data:\n","tp: 0.788462 [41]\n","fp: 0.307692 [12]\n","tn: 0.692308 [27]\n","fn: 0.211538 [11]\n"]}],"source":["trial.evaluate()\n","print('Trial statistics based on Ground-Truth from development set data:')\n","print(f'tp: {trial.true_positives:.06f} [{len(trial.true_positives_list)}]')\n","print(f'fp: {trial.false_positives:.06f} [{len(trial.false_positives_list)}]')\n","print(f'tn: {trial.true_negatives:.06f} [{len(trial.true_negatives_list)}]')\n","print(f'fn: {trial.false_negatives:.06f} [{len(trial.false_negatives_list)}]')"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"ximXdpDkJ3xS","outputId":"dde53833-1f7b-4b4e-9f0a-66682b58bde0","pycharm":{"name":"#%%\n"}},"outputs":[{"data":{"text/plain":["0.780952380952381"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["trial.f_measure"]},{"cell_type":"markdown","metadata":{"id":"32c-kCWqJ3xT","pycharm":{"name":"#%% md\n"}},"source":["### Save Test predictions\n","You will be evaluated on dev and test predictions, over a hidden ground truth."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"t1qkfFDrJ3xT","outputId":"8bed2f96-0065-4893-e9ac-21d8e44ced53","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Column names are: goodreads.ID, barnes_and_nobles.ID\n","Processed 90 lines.\n"]}],"source":["test_set_file = dir_ + 'test.csv'\n","test = []\n","with open(test_set_file, encoding='utf-8', errors=\"replace\") as csv_file:\n","    csv_reader = csv.reader(csv_file, delimiter=',')\n","    line_count = 0\n","    for row in csv_reader:\n","        if len(row) <= 1:\n","            continue\n","        if line_count == 0:\n","            columns = row\n","            line_count += 1\n","        else:\n","            test.append(row)\n","    print(f'Column names are: {\", \".join(columns)}')\n","    print(f'Processed {len(test)} lines.')"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"UfC2TKISJ3xT","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["predictions = []\n","# print(len(ds1.generate_dataframe()))\n","# print(len(ds2.generate_dataframe()))\n","# print(\"----------\")\n","for id1, id2 in test:\n","    # print(id1, '\\t', id2)\n","    # print(\"=======\")\n","    # print(r1.name_string, '\\t', r2.name_string)\n","    r1 = ds1.get_record(id1)\n","    r2  = ds2.get_record(id2)\n","    result, confidence = rule_based_method(r1, r2, print_flag=True)\n","        # print(\"R1: \", r1.name_string, \" \\t \", r1.first_author, \" , \", r1.second_author, \" , \", r1.third_author)\n","        # print(\"R2: \", r2.name_string, \" \\t \", r2.first_author, \" , \", r2.second_author, \" , \", r2.third_author)\n","        # print(result, \" , \", confidence)\n","        # print(\"=======\")\n","\n","    predictions.append((r1.id, r2.id, result, confidence))"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"4UCWB2Q6J3xT","outputId":"5a2d2a1d-3c4a-4d87-da59-89cc94ee6f8b","pycharm":{"name":"#%%\n"}},"outputs":[{"data":{"text/plain":["(90, 3967, 3701)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["len(predictions), len(ds1.generate_dataframe()), len(ds2.generate_dataframe())"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"UnxUzg4WJ3xT","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["with open(dir_ + 'Devyan_Biswas_predictions.csv', mode='w') as file:\n","    writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","    writer.writerow(['goodreads.ID','barnes_and_nobles.ID', 'prediction', 'confidence'])\n","    for row in predictions:\n","        writer.writerow(row)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["with open(dir_ + 'Devyan_Biswas_valid_predictions.csv', mode='w') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(['goodreads.ID','barnes_and_nobles.ID'])\n","    for row in predictions:\n","        if row[2] == True:\n","            writer.writerow([row[0], row[1]])"]},{"cell_type":"markdown","metadata":{"id":"G7svOwNGJ3xT","pycharm":{"name":"#%% md\n"}},"source":["# Task 2: Using RDFLib for Knowledge Representation"]},{"cell_type":"markdown","metadata":{"id":"RDtcKE-kJ3xT","pycharm":{"name":"#%% md\n"}},"source":["RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information as graphs. RDFLib aims to be a pythonic RDF API, a Graph is a python collection of RDF Subject, Predicate,  Object Triples.\n","\n","This notebook introduces simple examples. You can also find additional information in the [official documenation](https://rdflib.readthedocs.io/en/stable/)."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"yQmhRDzUJ3xT","outputId":"488a0cab-2e34-4798-d316-57733f682733","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: rdflib in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (6.2.0)\n","Requirement already satisfied: pyparsing in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from rdflib) (3.0.9)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8.0\" in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from rdflib) (4.12.0)\n","Requirement already satisfied: isodate in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from rdflib) (0.6.1)\n","Requirement already satisfied: setuptools in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from rdflib) (41.2.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8.0\"->rdflib) (4.3.0)\n","Requirement already satisfied: zipp>=0.5 in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8.0\"->rdflib) (3.8.1)\n","Requirement already satisfied: six in /Users/devyanbiswas/Desktop/DSCI558/Homework/558/lib/python3.7/site-packages (from isodate->rdflib) (1.16.0)\n","\u001b[33mWARNING: You are using pip version 19.2.3, however version 22.2.2 is available.\n","You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["! pip install rdflib"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"XSlFv0BhJ3xT","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["from rdflib import Graph, URIRef, Literal, XSD, Namespace, RDF, BNode"]},{"cell_type":"markdown","metadata":{"id":"M-loxb1-J3xU","pycharm":{"name":"#%% md\n"}},"source":["Let's define some namespaces:"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"z0SzNXOhJ3xU","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# FOAF = Namespace('http://xmlns.com/foaf/0.1/')\n","# MYNS = Namespace('http://dsci558.org/myfakenamespace#')"]},{"cell_type":"markdown","metadata":{"id":"k7Rul4D1J3xU","pycharm":{"name":"#%% md\n"}},"source":["We can create a graph:"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"rSQH4Uo5J3xU","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# my_kg = Graph()\n","# my_kg.bind('myns', MYNS)\n","# my_kg.bind('foaf', FOAF)"]},{"cell_type":"markdown","metadata":{"id":"wAY1RbdIJ3xU","pycharm":{"name":"#%% md\n"}},"source":["Define a URI, then add a simple triple to the graph:"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"3I0EK2ipJ3xU","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# node_uri = URIRef(MYNS['dsci_558'])\n","# my_kg.add((node_uri, RDF.type, MYNS['course']))"]},{"cell_type":"markdown","metadata":{"id":"oof8rvbjJ3xU","pycharm":{"name":"#%% md\n"}},"source":["Add an additional triple (which describes the same subject, `node_uri`):"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"ljxEhaLeJ3xU","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# my_kg.add((node_uri, FOAF['name'], Literal('Building Knowledge Graphs')))"]},{"cell_type":"markdown","metadata":{"id":"ed_XbkFbJ3xU","pycharm":{"name":"#%% md\n"}},"source":["And now let's dump our graph triples into some `ttl` file:"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"n2MI9-KKJ3xU","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# my_kg.serialize(dir_ + 'sample_graph.ttl', format=\"turtle\")"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"NIfJgmAvJ3xU","outputId":"f243c52a-40d9-4950-f45e-cc7fbbe0e22e","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["head: sample_graph.ttl: No such file or directory\n"]}],"source":["!head sample_graph.ttl"]},{"cell_type":"markdown","metadata":{},"source":["## Ok, for 2.1, gotta read in the valid pairs' info first"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"dvnuzp4OJ3xU","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# Reading in the valid pairs\n","preds = dir_ + 'Devyan_Biswas_valid_predictions.csv'\n","valid_pairs = []\n","with open(preds, encoding='utf-8', errors=\"replace\") as csv_file:\n","    csv_reader = csv.reader(csv_file, delimiter=',')\n","    line_count = 0\n","    for row in csv_reader:\n","        if len(row) <= 1:\n","            continue\n","        if line_count == 0:\n","            columns = row\n","            line_count += 1\n","        else:\n","            valid_pairs.append(row)\n","# print(len(valid_pairs), valid_pairs)\n"]},{"cell_type":"markdown","metadata":{},"source":["Now, what we gotta do is get the rows from the goodreads and barnes and nobles csv's and get the appropriate columns, then\n","<br>\n","filter based on id's and pair em up in a data struct\n","<br>\n","tbh i got kinda lazy, so I just started using pandas dataframes to do this lol"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["gr_pd = pd.read_csv('./goodreads.csv', sep=',', usecols = ['ID', 'Title','Description','ISBN', 'ISBN13', 'PageCount', 'FirstAuthor', 'SecondAuthor', 'ThirdAuthor', 'Rating' , 'NumberofRatings', 'NumberofReviews', 'Publisher', 'PublishDate', 'Format', 'Language'], header=0) \n","ban_pd = pd.read_csv('./barnes_and_nobles.csv', sep=',', usecols = ['ID', 'Title','Author1', 'Author2', 'Author3', 'Publisher' , 'ISBN13', 'PublicationDate', 'Productdimensions', 'Salesrank', 'Ratingscount', 'Ratingvalue', 'Paperbackprice', 'Hardcoverprice', 'Nookbookprice', 'Audiobookprice'], header=0) "]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["valid_gr_ids = [item[0] for item in valid_pairs]\n","valid_ban_ids = [item[1] for item in valid_pairs]"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["valid_gr_pd = gr_pd.iloc[valid_gr_ids]\n","valid_ban_pd = ban_pd.iloc[valid_ban_ids]"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["valid_gr_tuples = list(valid_gr_pd.itertuples(index=False))\n","valid_ban_tuples = list(valid_ban_pd.itertuples(index=False))"]},{"cell_type":"markdown","metadata":{},"source":["We're only gonna be adding one entry/node per pair, but add the attributes from BOTH datasets as per HW2 documentation"]},{"cell_type":"markdown","metadata":{},"source":["This part's gonna be very hardcode-y, but eh "]},{"cell_type":"markdown","metadata":{},"source":["I am definind the URI as the concat of the two id's. Why? Because im pressed for time and was running into an issue with making\n","<br>\n","the title part of the URI\n","<br>\n","You'll note that overall I give preference to stats from the goodread dataset. personal choice tbh, nothing more\n","<br>\n","If I had more time, I think a good approach would be to even create two separate nodes from GR and BAN, do respective dataproc,\n","<br>\n","then just add a link to the top levels of each entry. But this was, somehow, simpler/more straightforward lell"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["SCHEMA = Namespace(\"https://schema.org/\")\n","FOAF = Namespace('http://xmlns.com/foaf/0.1/')\n","MYNS = Namespace('http://dsci558.org/myfakenamespace#')"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["my_kg = Graph()\n","my_kg.bind('myns', MYNS)\n","my_kg.bind('foaf', FOAF)\n","my_kg.bind('schema', SCHEMA)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["57\n"]}],"source":["import math\n","counter = 0\n","\n","for gr, ban in zip(valid_gr_tuples, valid_ban_tuples):\n","    counter = counter + 1\n","    # print(gr)\n","    # print(\"=====\")\n","    # print(ban)\n","\n","    # Node URI definition\n","    node_id = str(gr[0])+ \".\" + str(ban[0])\n","    node_URI = URIRef(SCHEMA[node_id])\n","    my_kg.add((node_URI, RDF.type, SCHEMA.Book))\n","\n","    # Node title definition\n","    if len(gr[1]) > len(ban[1]):\n","        book_name = gr[1]\n","    else:\n","        book_name = ban[1]\n","\n","    my_kg.add((node_URI, SCHEMA.name, Literal(book_name)))\n","\n","    # Adding the abstract\n","    abstract_text = gr[2]\n","    if len(abstract_text) > 1:\n","        my_kg.add((node_URI, SCHEMA.abstract, Literal(abstract_text)))\n","    else:\n","        my_kg.add((node_URI, SCHEMA.abstract, BNode()))\n","    \n","    # Adding the ISBN and ISBN13\n","\n","    # This causes nans to be added instead of blank nodes, but dont have time to resolve edge cases\n","    if gr[3] == \"nan\":\n","        my_kg.add((node_URI, SCHEMA.isbn, BNode()))\n","    else:\n","        my_kg.add((node_URI, SCHEMA.isbn, Literal(gr[3], datatype=SCHEMA.ISBN)))\n","\n","    if gr[4] == ' ':\n","        if ban[6] ==' ':\n","            my_kg.add((node_URI, SCHEMA.isbn13, BNode()))\n","        else:\n","            my_kg.add((node_URI, SCHEMA.isbn13, Literal(ban[6], datatype=SCHEMA.ISBN)))\n","    else:\n","        my_kg.add((node_URI, SCHEMA.isbn13, Literal(gr[4], datatype=SCHEMA.ISBN)))\n","\n","\n","    # Adding the bag of authors:\n","    # NOTE: This is NOT A CORRECT IMPLEMENTATION I am just low on time\n","    # If more time, would do a similarity check for strings and then add the three auths only but this should be aight\n","    auths = list()\n","    if gr[6] != ' ':\n","        auths.append(gr[6])\n","    if gr[7] != ' ':\n","        auths.append(gr[7])\n","    if gr[8] != ' ':\n","        auths.append(gr[8])\n","    if ban[2] != ' ' and ban[2] not in auths:\n","        auths.append(ban[2])\n","    if ban[3] != ' ' and ban[3] not in auths:\n","        auths.append(ban[3])\n","    if ban[4] != ' ' and ban[4] not in auths:\n","        auths.append(ban[4])\n","\n","    bag = BNode()\n","    my_kg.add((node_URI, SCHEMA.author, bag))\n","    my_kg.add((bag, RDF.type, RDF.Bag))\n","    for auth in auths:\n","        my_kg.add((bag, SCHEMA.author, Literal(auth, datatype=SCHEMA.author)))\n","    \n","    # Adding ratings (not including rating count tho)\n","    if gr[9]==\"nan\":\n","        if ban[11] == ' ':\n","            my_kg.add((node_URI, SCHEMA.rating, BNode()))\n","        rating = str(ban[11])\n","    else:\n","        rating = str(gr[9])\n","    \n","    my_kg.add((node_URI, SCHEMA.rating, Literal(rating, datatype=SCHEMA.contentRating)))\n","    \n","    # Adding Publisher Info\n","    publisher = \"\"\n","    if gr[12] != ' ':\n","        publisher = publisher + gr[12] + \", \"\n","    if ban[5] != ' ':\n","        publisher = publisher + ban[5] \n","\n","    my_kg.add((node_URI, SCHEMA.publisher, Literal(publisher, datatype=SCHEMA.publisher)))\n","    \n","    if gr[13] == ' ':\n","        if ban[7] == ' ':\n","            my_kg.add((node_URI, SCHEMA.datePublished, BNode()))\n","        else:\n","            my_kg.add((node_URI, SCHEMA.datePublished, Literal(ban[7], datatype=SCHEMA.datePublished)))\n","    else:\n","        my_kg.add((node_URI, SCHEMA.datePublished, Literal(gr[13], datatype=SCHEMA.datePublished)))\n","\n","    # Lastly, I just want to have the prices from the BAN record\n","    12, 13, 14, 15\n","    papercost = ban[12]\n","    hardcost = ban[13]\n","    nookcost = ban[14]\n","    audiocost = ban[15]\n","    \n","    price_bag = BNode()\n","    my_kg.add((node_URI, SCHEMA.price, price_bag))\n","    my_kg.add((price_bag, RDF.type, RDF.Bag))\n","    if papercost == ' ':\n","        my_kg.add((price_bag, SCHEMA.paper_price,BNode()))\n","    else:\n","        my_kg.add((price_bag, SCHEMA.paper_price, Literal(papercost, datatype=SCHEMA.price)))\n","\n","    if hardcost == ' ':\n","        my_kg.add((price_bag, SCHEMA.hard_price,BNode()))\n","    else:\n","        my_kg.add((price_bag, SCHEMA.hard_price, Literal(hardcost, datatype=SCHEMA.price)))\n","\n","    if nookcost == ' ':\n","        my_kg.add((price_bag, SCHEMA.nook_price,BNode()))\n","    else:\n","        my_kg.add((price_bag, SCHEMA.nook_price, Literal(nookcost, datatype=SCHEMA.price)))\n","\n","    if audiocost == ' ':\n","        my_kg.add((price_bag, SCHEMA.audio_price,BNode()))\n","    else:\n","        my_kg.add((price_bag, SCHEMA.audio_price, Literal(audiocost, datatype=SCHEMA.price)))\n","\n","    # break\n","\n","print(counter)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["<Graph identifier=Na78dadcf44b7439ebdff37f1be1da30c (<class 'rdflib.graph.Graph'>)>"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["my_kg.serialize(dir_ + 'Devyan_Biswas_model.ttl', format=\"turtle\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.7.5 ('558': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"vscode":{"interpreter":{"hash":"9283565c2edb4b4d683f503e764f8eab77581b5966062eabe376875498269cc6"}}},"nbformat":4,"nbformat_minor":0}
